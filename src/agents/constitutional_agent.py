import asyncio
from typing import Any

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable

from ..utils.cross_encoder_reranker import CrossEncoderReRanker

# Template de prompt para gerar varia√ß√µes da pergunta original
MULTI_QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""Voc√™ √© um assistente de IA especialista em direito. Sua tarefa √© gerar 3 vers√µes diferentes da pergunta do usu√°rio para recuperar documentos relevantes de um banco de dados vetorial.

Ao gerar m√∫ltiplas perspectivas sobre a pergunta do usu√°rio, seu objetivo √© ajudar o usu√°rio a superar algumas das limita√ß√µes da busca por similaridade baseada em dist√¢ncia.

Forne√ßa as perguntas alternativas separadas por quebras de linha. N√£o numere as perguntas.

Pergunta Original: {question}""",
)


class ConstitutionalAgent:
    """Agente especializado em Direito Constitucional com busca Multi-Query."""

    def __init__(self, domain: str, retriever: Any, llm: Runnable):
        self.name = "constitutional_agent"
        self.domain = domain
        self.retriever = retriever
        self.llm = llm
        self.final_answer_prompt = self._create_final_answer_prompt()
        self.reranker = CrossEncoderReRanker()

    def _create_final_answer_prompt(self) -> PromptTemplate:
        """Cria o prompt final para gerar a resposta com base no contexto."""
        return PromptTemplate(
            input_variables=["context", "question", "chat_history"],
            template="""Voc√™ √© um assistente especialista em Direito Constitucional brasileiro. Sua tarefa √© responder √† pergunta do usu√°rio de forma clara e concisa, baseando-se exclusivamente nos trechos de documentos fornecidos no contexto e considerando o hist√≥rico da conversa.

**Instru√ß√µes Importantes:**
1.  **Foque na Pergunta Atual:** Use o hist√≥rico da conversa para entender o contexto, mas sua resposta deve focar em responder diretamente √† √∫ltima pergunta do usu√°rio.
1.  **Sintetize a Informa√ß√£o:** Os documentos no contexto s√£o fragmentos. Sua principal tarefa √© conectar as informa√ß√µes de m√∫ltiplos fragmentos para construir uma resposta completa.
2.  **Seja Exclusivo:** Responda APENAS com base no contexto. N√£o utilize nenhum conhecimento pr√©vio.
3.  **Seja Honesto:** Se, ap√≥s analisar todos os fragmentos, a informa√ß√£o para responder √† pergunta n√£o estiver presente, informe que n√£o foi poss√≠vel encontrar uma resposta conclusiva nos documentos consultados.

Contexto:
{context}

Hist√≥rico da Conversa:
{chat_history}

Pergunta: {question}

Resposta:""",
        )

    async def _generate_queries(self, question: str) -> list[str]:
        """Gera perguntas alternativas usando o LLM."""
        try:
            print(f"   [{self.name}] Gerando varia√ß√µes da pergunta...")
            generate_queries_chain = MULTI_QUERY_PROMPT | self.llm | StrOutputParser()
            response = await generate_queries_chain.ainvoke({"question": question})

            # Adiciona a pergunta original e as geradas, filtrando linhas vazias
            all_queries = [question] + [
                q.strip() for q in response.split("\n") if q.strip()
            ]
            print(f"   [{self.name}] Total de perguntas para busca: {len(all_queries)}")
            return all_queries
        except Exception as e:
            print(
                f"   ‚ö†Ô∏è [{self.name}] Erro ao gerar perguntas: {e}. Usando apenas a pergunta original."
            )
            return [question]

    async def _get_unique_documents(self, queries: list[str]) -> list[Document]:
        """Busca documentos para m√∫ltiplas perguntas e remove duplicatas."""
        print(f"   [{self.name}] Executando busca com {len(queries)} perguntas...")
        # Cria tarefas para buscar documentos para cada pergunta em paralelo
        tasks = [self.retriever.ainvoke(query) for query in queries]
        # Aguarda todas as buscas
        results = await asyncio.gather(*tasks)

        # Junta todos os documentos de todas as buscas
        all_docs = [doc for sublist in results for doc in sublist]

        # Remove documentos duplicados usando o conte√∫do e a fonte como chave
        unique_docs_map = {}
        for doc in all_docs:
            key = (doc.page_content, doc.metadata.get("source", ""))
            if key not in unique_docs_map:
                unique_docs_map[key] = doc

        unique_docs = list(unique_docs_map.values())
        print(f"   [{self.name}] Documentos √∫nicos encontrados: {len(unique_docs)}")
        return unique_docs

    async def _rerank_documents(
        self, query: str, documents: list[Document]
    ) -> list[Document]:
        """
        ‚úÖ v0.2: Usa o CrossEncoder para reordenar os documentos.
        Esta fun√ß√£o √© executada em um thread separado para n√£o bloquear o loop de eventos.
        """
        if not documents:
            return []

        print(
            f"   [{self.name}] Reordenando {len(documents)} documentos com CrossEncoder..."
        )

        loop = asyncio.get_event_loop()
        # A l√≥gica de re-ranking agora √© chamada diretamente no executor.
        reranked_docs: list[Document] = await loop.run_in_executor(  # type: ignore
            None, self.reranker.rerank, query, documents
        )
        print(
            f"   [{self.name}] Documentos reordenados e selecionados: {len(reranked_docs)}"
        )
        return reranked_docs

    async def invoke(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """
        Invoca o agente com a l√≥gica Multi-Query.
        """
        # ‚úÖ v0.3.1: O agente agora recebe uma pergunta j√° reescrita e aut√¥noma.
        # A pergunta original √© mantida para a resposta final.
        standalone_query = inputs["query"]
        original_query = inputs.get("original_query", standalone_query)
        conversation_history = inputs.get("conversation_history", [])

        print(f"   üöÄ Agente '{self.name}' invocado para o dom√≠nio '{self.domain}'")

        # 1. Gerar m√∫ltiplas perguntas (usando a pergunta aut√¥noma)
        queries = await self._generate_queries(standalone_query)

        # 2. Recuperar documentos √∫nicos usando todas as perguntas
        documents = await self._get_unique_documents(queries)

        # 3. ‚úÖ v0.2: Reordena e seleciona os melhores documentos com CrossEncoder
        final_documents = await self._rerank_documents(standalone_query, documents)

        if not final_documents:
            return {
                "agent": self.name,
                "agent_domain": self.domain,
                "answer": "N√£o foram encontrados documentos relevantes para responder a esta pergunta.",
                "sources": [],
                "confidence": 0.1,
                "status": "no_documents",
            }

        # 4. Formatar o contexto e gerar a resposta final
        context = "\n\n---\n\n".join([doc.page_content for doc in final_documents])

        final_chain = self.final_answer_prompt | self.llm | StrOutputParser()

        # Formata o hist√≥rico para o prompt final
        formatted_history = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in conversation_history]
        )

        answer = await final_chain.ainvoke(
            {
                "context": context,
                "question": original_query,
                "chat_history": formatted_history,
            }
        )

        # Extrai as fontes dos documentos
        sources = list(
            set(doc.metadata.get("file_name", "N/A") for doc in final_documents)
        )

        return {
            "agent": self.name,
            "agent_domain": self.domain,
            "answer": answer,
            "sources": sources,
            "confidence": 0.75,  # Valor placeholder, podemos refinar depois
            "status": "success",
        }
